{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf720ba7-3de3-456e-ae27-93f6f68c4ca4",
   "metadata": {},
   "source": [
    "# Utilisation de la bibliotheque mediapipe pour detecter les doigts\n",
    "## Exemple de comptage des doigts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50691d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mediapipe opencv-python pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc24cdb-f110-4bb9-8432-b23e776fe1c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Step 3: Initialize MediaPipe Hands and Drawing Utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Step 4: Function to determine hand orientation\n",
    "def determine_hand(hand_landmarks):\n",
    "    # Check whether the pinky (20) is higher than the thumb (4) in the y-axis\n",
    "    if hand_landmarks[20].x > hand_landmarks[4].x:\n",
    "        return \"main droite\"  # Right hand\n",
    "    else:\n",
    "        return \"main gauche\"  # Left hand\n",
    "\n",
    "# Function to count raised fingers based on landmarks\n",
    "def count_fingers(hand_landmarks):\n",
    "    fingers = []\n",
    "    main = determine_hand(hand_landmarks)\n",
    "\n",
    "    # Thumb\n",
    "    if main == \"main droite\":\n",
    "        if hand_landmarks[4].x < hand_landmarks[3].x:  # Thumb is raised if to the left\n",
    "            fingers.append(1)\n",
    "        else:\n",
    "            fingers.append(0)\n",
    "    else:  # Left hand\n",
    "        if hand_landmarks[4].x > hand_landmarks[3].x:  # Thumb is raised if to the right\n",
    "            fingers.append(1)\n",
    "        else:\n",
    "            fingers.append(0)\n",
    "\n",
    "    # Other four fingers\n",
    "    for tip in [8, 12, 16, 20]:  # Index, middle, ring, pinky\n",
    "        if hand_landmarks[tip].y < hand_landmarks[tip - 2].y:  # Finger is raised\n",
    "            fingers.append(1)\n",
    "        else:\n",
    "            fingers.append(0)\n",
    "\n",
    "    return fingers.count(1)\n",
    "\n",
    "# Step 5: Webcam setup and hand sign detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a later selfie-view display\n",
    "        # Convert the BGR image to RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Process the image and detect hand landmarks\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Convert the image back to BGR for OpenCV processing\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the image\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Count fingers\n",
    "                num_fingers = count_fingers(hand_landmarks.landmark)\n",
    "\n",
    "                # Display the number of fingers\n",
    "                cv2.putText(image, f'Fingers: {num_fingers}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display the final output\n",
    "        cv2.imshow('Hand Sign Detection', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:  # Exit when 'Escape' key is pressed\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991bc8f-dc8b-4873-b2fc-e9d03e048652",
   "metadata": {},
   "source": [
    "# Example control mouse with finger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mediapipe opencv-python pycaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb84490-4a69-4fa2-a00a-73d12456679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "\n",
    "# Step 3: Initialize MediaPipe Hands and Drawing Utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Step 4: Webcam setup and finger-based mouse control\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display\n",
    "        # Convert the BGR image to RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        #image en mode read-only pour un processing rapide de chaque image\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Process the image and detect hand landmarks\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Convert the image back to BGR for OpenCV processing\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # If hand landmarks are detected, move the mouse based on index finger position\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the image\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Get the index finger tip coordinates (landmark 8)\n",
    "                index_finger_tip = hand_landmarks.landmark[8]\n",
    "                if hand_landmarks.landmark[8].y<hand_landmarks.landmark[5].y  or hand_landmarks.landmark[8].y<hand_landmarks.landmark[6].y:\n",
    "                    h, w, c = image.shape\n",
    "                    finger_x = int(index_finger_tip.x * w)\n",
    "                    finger_y = int(index_finger_tip.y * h)\n",
    "    \n",
    "                    # Convert the finger coordinates from webcam resolution to screen resolution\n",
    "                    screen_x = np.interp(finger_x, [0, w], [0, screen_width])\n",
    "                    screen_y = np.interp(finger_y, [0, h], [0, screen_height])\n",
    "    \n",
    "                    # Move the mouse cursor to the finger position\n",
    "                    pyautogui.moveTo(screen_x, screen_y)\n",
    "    \n",
    "                    # Draw a circle on the index finger tip\n",
    "                    cv2.circle(image, (finger_x, finger_y), 10, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "        # Display the webcam feed with hand tracking\n",
    "        cv2.imshow('Finger Mouse Control', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123730f-d496-42f5-ac91-3c889cfc9d98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Example volume control with fingers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mediapipe opencv-python pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fcc427-b034-41d2-ac64-3aabc47c234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "import numpy as np\n",
    "\n",
    "# Step 3: Initialize MediaPipe Hands and pycaw for volume control\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize pycaw for volume control\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "# Get volume range (min, max)\n",
    "vol_range = volume.GetVolumeRange()\n",
    "min_vol = vol_range[0]\n",
    "max_vol = vol_range[1]\n",
    "\n",
    "# Step 4: Function to calculate the distance between two points\n",
    "def calculate_distance(x1, y1, x2, y2):\n",
    "    return math.hypot(x2 - x1, y2 - y1)\n",
    "\n",
    "# Step 5: Webcam setup for hand detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Deadzone for setting volume to 0\n",
    "deadzone = 20  # Distance in pixels considered \"touching\" (you can adjust this)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display\n",
    "        # Convert the BGR image to RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Process the image and detect hand landmarks\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Convert the image back to BGR for OpenCV processing\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # If hand landmarks are detected, process them\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the image\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Get thumb tip and index finger tip coordinates\n",
    "                thumb_tip = hand_landmarks.landmark[4]   # Thumb tip\n",
    "                index_finger_tip = hand_landmarks.landmark[8]  # Index finger tip\n",
    "\n",
    "                # Get height and width of the image\n",
    "                h, w, c = image.shape\n",
    "\n",
    "                # Convert normalized landmarks to pixel values\n",
    "                thumb_x, thumb_y = int(thumb_tip.x * w), int(thumb_tip.y * h)\n",
    "                index_x, index_y = int(index_finger_tip.x * w), int(index_finger_tip.y * h)\n",
    "                print('Pt 4',hand_landmarks.landmark[4].y)\n",
    "                print('Pt 10',hand_landmarks.landmark[10].y)\n",
    "                if hand_landmarks.landmark[8].y<hand_landmarks.landmark[5].y:\n",
    "                    # Calculate the distance between thumb and index finger\n",
    "                    distance = calculate_distance(thumb_x, thumb_y, index_x, index_y)\n",
    "\n",
    "                    # Set volume to 0 if the distance is within the deadzone\n",
    "                    if distance < deadzone:\n",
    "                        vol = min_vol  # Set to minimum volume\n",
    "                    else:\n",
    "                        # Map the distance to volume (0 when touching, max when far apart)\n",
    "                        max_distance = 200  # Adjust this based on your hand size and webcam resolution\n",
    "                        vol = np.interp(distance, [deadzone, max_distance], [min_vol, max_vol])\n",
    "\n",
    "                    # Set the system volume based on the calculated volume\n",
    "                    volume.SetMasterVolumeLevel(vol, None)\n",
    "\n",
    "                    # Draw circles on thumb and index finger tips\n",
    "                    cv2.circle(image, (thumb_x, thumb_y), 10, (255, 0, 0), cv2.FILLED)\n",
    "                    cv2.circle(image, (index_x, index_y), 10, (255, 0, 0), cv2.FILLED)\n",
    "\n",
    "                    # Draw a line between thumb and index finger\n",
    "                    cv2.line(image, (thumb_x, thumb_y), (index_x, index_y), (0, 255, 0), 3)\n",
    "\n",
    "                    # Display the distance (for debugging purposes)\n",
    "                    cv2.putText(image, f'Distance: {int(distance)}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "                    # Display the volume level (for debugging purposes)\n",
    "                    cv2.putText(image, f'Volume: {int(np.interp(vol, [min_vol, max_vol], [0, 100]))}%', \n",
    "                                (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        # Display the webcam feed\n",
    "        cv2.imshow('Volume Control with Hand Gestures', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32890911",
   "metadata": {},
   "source": [
    "# Volume avec angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1a579ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Initialize MediaPipe Hands and pycaw for volume control\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize pycaw for volume control\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "# Get volume range (min, max)\n",
    "vol_range = volume.GetVolumeRange()\n",
    "min_vol = vol_range[0]\n",
    "max_vol = vol_range[1]\n",
    "\n",
    "# Step 2: Function to calculate angle between three points\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)  # First point\n",
    "    b = np.array(b)  # Middle point (vertex)\n",
    "    c = np.array(c)  # Third point\n",
    "\n",
    "    # Calculate vectors\n",
    "    ab = a - b\n",
    "    bc = c - b\n",
    "\n",
    "    # Dot product and magnitudes\n",
    "    dot_product = np.dot(ab, bc)\n",
    "    magnitude = np.linalg.norm(ab) * np.linalg.norm(bc)\n",
    "\n",
    "    # Calculate angle in radians and convert to degrees\n",
    "    angle = np.arccos(dot_product / magnitude)\n",
    "    return np.degrees(angle)\n",
    "\n",
    "# Step 3: Webcam setup for hand detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display\n",
    "        # Convert the BGR image to RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Process the image and detect hand landmarks\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Convert the image back to BGR for OpenCV processing\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Step 4: Process detected landmarks\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the image\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Get wrist, thumb tip, and index finger tip coordinates\n",
    "                wrist = hand_landmarks.landmark[0]  # Wrist\n",
    "                thumb_tip = hand_landmarks.landmark[4]  # Thumb tip\n",
    "                index_tip = hand_landmarks.landmark[8]  # Index finger tip\n",
    "\n",
    "                # Get height and width of the image\n",
    "                h, w, c = image.shape\n",
    "\n",
    "                # Convert normalized landmarks to pixel coordinates\n",
    "                wrist_coords = [int(wrist.x * w), int(wrist.y * h)]\n",
    "                thumb_coords = [int(thumb_tip.x * w), int(thumb_tip.y * h)]\n",
    "                index_coords = [int(index_tip.x * w), int(index_tip.y * h)]\n",
    "\n",
    "                if hand_landmarks.landmark[8].y<hand_landmarks.landmark[5].y:\n",
    "                    \n",
    "                    # Calculate the angle\n",
    "                    angle = calculate_angle(thumb_coords, wrist_coords, index_coords)\n",
    "\n",
    "                    # Map the angle to volume range\n",
    "                    max_angle = 45  # Max angle for fully extended fingers\n",
    "                    vol = np.interp(angle, [10, max_angle], [min_vol, max_vol])\n",
    "\n",
    "                    # Set system volume\n",
    "                    volume.SetMasterVolumeLevel(vol, None)\n",
    "\n",
    "                    # Step 5: Visual feedback\n",
    "                    # Display angle\n",
    "                    cv2.putText(image, f'Angle: {int(angle)} deg', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "                    # Display volume level\n",
    "                    cv2.putText(image, f'Volume: {int(np.interp(vol, [min_vol, max_vol], [0, 100]))}%', \n",
    "                                (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "                    # Draw circles and lines on the image\n",
    "                    cv2.circle(image, tuple(thumb_coords), 10, (255, 0, 0), cv2.FILLED)\n",
    "                    cv2.circle(image, tuple(index_coords), 10, (255, 0, 0), cv2.FILLED)\n",
    "                    cv2.circle(image, tuple(wrist_coords), 10, (0, 255, 0), cv2.FILLED)\n",
    "                    cv2.line(image, tuple(wrist_coords), tuple(thumb_coords), (0, 255, 0), 3)\n",
    "                    cv2.line(image, tuple(wrist_coords), tuple(index_coords), (0, 255, 0), 3)\n",
    "\n",
    "        # Display the webcam feed\n",
    "        cv2.imshow('Volume Control with Hand Gestures', image)\n",
    "\n",
    "        # Press 'ESC' to exit\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a06c7d",
   "metadata": {},
   "source": [
    "# Example volume control with fingers v1.0 (sans distorion de prespective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d0f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Hands and Drawing Utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize pycaw for volume control\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "# Get volume range (min, max)\n",
    "min_vol, max_vol, _ = volume.GetVolumeRange()\n",
    "\n",
    "# Function to calculate the distance between two points\n",
    "def calculate_distance(x1, y1, x2, y2):\n",
    "    return math.hypot(x2 - x1, y2 - y1)\n",
    "\n",
    "# Webcam setup\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Deadzone and max distance for normalized mapping\n",
    "DEADZONE = 0.1  # Normalized distance considered \"touching\"\n",
    "MAX_NORMALIZED_DISTANCE = 0.3  # Adjust based on hand size and resolution\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip and process the frame\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        rgb_frame.flags.writeable = False\n",
    "\n",
    "        # Detect hand landmarks\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        # Convert frame back to BGR for display\n",
    "        rgb_frame.flags.writeable = True\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        # Process detected hand landmarks\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw landmarks and connections\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Get coordinates for thumb tip, index finger tip, and wrist\n",
    "                thumb_tip = hand_landmarks.landmark[4]\n",
    "                index_tip = hand_landmarks.landmark[8]\n",
    "                wrist = hand_landmarks.landmark[0]\n",
    "\n",
    "                # Calculate absolute distances (pixel values)\n",
    "                thumb_x, thumb_y = int(thumb_tip.x * w), int(thumb_tip.y * h)\n",
    "                index_x, index_y = int(index_tip.x * w), int(index_tip.y * h)\n",
    "                wrist_x, wrist_y = int(wrist.x * w), int(wrist.y * h)\n",
    "\n",
    "                # Calculate reference distance (wrist to middle finger tip)\n",
    "                ref_distance = calculate_distance(wrist.x, wrist.y, hand_landmarks.landmark[12].x, hand_landmarks.landmark[12].y)\n",
    "\n",
    "                # Calculate normalized distance between thumb and index finger\n",
    "                normalized_distance = calculate_distance(thumb_tip.x, thumb_tip.y, index_tip.x, index_tip.y) / ref_distance\n",
    "\n",
    "                # Set volume based on normalized distance\n",
    "                if normalized_distance < DEADZONE:\n",
    "                    vol = min_vol  # Minimum volume\n",
    "                else:\n",
    "                    vol = np.interp(normalized_distance, [DEADZONE, MAX_NORMALIZED_DISTANCE], [min_vol, max_vol])\n",
    "\n",
    "                # Apply volume change\n",
    "                volume.SetMasterVolumeLevel(vol, None)\n",
    "\n",
    "                # Draw thumb, index markers, and line between them\n",
    "                cv2.circle(frame, (thumb_x, thumb_y), 10, (255, 0, 0), cv2.FILLED)\n",
    "                cv2.circle(frame, (index_x, index_y), 10, (255, 0, 0), cv2.FILLED)\n",
    "                cv2.line(frame, (thumb_x, thumb_y), (index_x, index_y), (0, 255, 0), 3)\n",
    "\n",
    "                # Display normalized distance and volume for debugging\n",
    "                volume_percentage = int(np.interp(vol, [min_vol, max_vol], [0, 100]))\n",
    "                cv2.putText(frame, f'Norm Dist: {normalized_distance:.2f}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            1, (255, 255, 255), 2)\n",
    "                cv2.putText(frame, f'Volume: {volume_percentage}%', (10, 100), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            1, (255, 255, 255), 2)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Volume Control with Hand Gestures', frame)\n",
    "\n",
    "        # Exit on pressing 'Esc'\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21be3b2e-ce8d-4c06-9765-5d48ba4ffa29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Swipe up for open apps, down for desktop, left right for switching apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1915d37-3a61-4249-90f2-00df7d5abe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "\n",
    "# Step 3: Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Step 4: Webcam setup for hand detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Gesture recognition variables\n",
    "swipe_threshold = 100  # Distance in pixels to recognize a swipe\n",
    "last_swipe_time = 0  # Time of last swipe\n",
    "swipe_timeout = 1  # Time in seconds to limit swipes\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    last_position = None  # To track the last hand position\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Process the image and detect hand landmarks\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Convert the image back to BGR for OpenCV processing\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # If hand landmarks are detected, process them\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the image\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Get index finger tip coordinates\n",
    "                index_finger_tip = hand_landmarks.landmark[8]  # Index finger tip\n",
    "\n",
    "                # Get height and width of the image\n",
    "                h, w, c = image.shape\n",
    "\n",
    "                # Convert normalized landmark to pixel values\n",
    "                index_x, index_y = int(index_finger_tip.x * w), int(index_finger_tip.y * h)\n",
    "\n",
    "                # Calculate the swipe direction based on previous position\n",
    "                if last_position is not None:\n",
    "                    distance_y = last_position[1] - index_y  # Difference in Y position\n",
    "                    distance_x = last_position[0] - index_x  # Difference in X position\n",
    "\n",
    "                    # Check if the swipe distance is significant\n",
    "                    if abs(distance_y) > swipe_threshold:\n",
    "                        current_time = time.time()\n",
    "                        if current_time - last_swipe_time > swipe_timeout:\n",
    "                            last_swipe_time = current_time\n",
    "                            if distance_y > swipe_threshold:  # Swipe Up\n",
    "                                pyautogui.hotkey('win', 'tab')  # Show all apps\n",
    "                            elif distance_y < -swipe_threshold:  # Swipe Down\n",
    "                                pyautogui.hotkey('win', 'd')  # Show desktop\n",
    "\n",
    "                    # Check left/right swipe\n",
    "                    if abs(distance_x) > swipe_threshold:\n",
    "                        current_time = time.time()\n",
    "                        if current_time - last_swipe_time > swipe_timeout:\n",
    "                            last_swipe_time = current_time\n",
    "                            if distance_x > swipe_threshold:  # Swipe Right\n",
    "                                pyautogui.hotkey('alt', 'tab')  # Switch to the next app\n",
    "                            elif distance_x < -swipe_threshold:  # Swipe Left\n",
    "                                pyautogui.hotkey('shift', 'alt', 'tab')  # Switch to the previous app\n",
    "\n",
    "                # Update the last position\n",
    "                last_position = (index_x, index_y)\n",
    "\n",
    "        # Display the webcam feed\n",
    "        cv2.imshow('Gesture Control for Apps', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75565ca1-cff1-4333-95aa-543913004bc2",
   "metadata": {},
   "source": [
    "# Pong game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db54649-2019-4fb7-b4ad-b18a8f81fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialize Mediapipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Pong game parameters\n",
    "WIDTH, HEIGHT = 640, 480\n",
    "PADDLE_WIDTH, PADDLE_HEIGHT = 20, 100\n",
    "BALL_RADIUS = 10\n",
    "PADDLE_SPEED = 20\n",
    "ball_speed_x, ball_speed_y = 7, 7\n",
    "\n",
    "# Initialize paddle and ball positions\n",
    "player_y = HEIGHT // 2 - PADDLE_HEIGHT // 2\n",
    "ball_x, ball_y = WIDTH // 2, HEIGHT // 2\n",
    "\n",
    "# Create a function to detect finger position\n",
    "def detect_finger_position(frame):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(frame_rgb)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Get index finger tip position (landmark 8)\n",
    "            x = int(hand_landmarks.landmark[8].x * WIDTH)\n",
    "            y = int(hand_landmarks.landmark[8].y * HEIGHT)\n",
    "            return x, y\n",
    "    return None, None\n",
    "\n",
    "# Game loop\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip frame for natural movement\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Detect finger position\n",
    "    finger_x, finger_y = detect_finger_position(frame)\n",
    "\n",
    "    # Move the paddle\n",
    "    if finger_y is not None:\n",
    "        player_y = finger_y - PADDLE_HEIGHT // 2\n",
    "\n",
    "    # Bound the paddle within the screen\n",
    "    if player_y < 0:\n",
    "        player_y = 0\n",
    "    if player_y + PADDLE_HEIGHT > HEIGHT:\n",
    "        player_y = HEIGHT - PADDLE_HEIGHT\n",
    "\n",
    "    # Ball movement\n",
    "    ball_x += ball_speed_x\n",
    "    ball_y += ball_speed_y\n",
    "\n",
    "    # Ball collision with top and bottom\n",
    "    if ball_y - BALL_RADIUS <= 0 or ball_y + BALL_RADIUS >= HEIGHT:\n",
    "        ball_speed_y *= -1\n",
    "\n",
    "    # Ball collision with player paddle\n",
    "    if (ball_x - BALL_RADIUS <= PADDLE_WIDTH and \n",
    "        player_y <= ball_y <= player_y + PADDLE_HEIGHT):\n",
    "        ball_speed_x *= -1\n",
    "\n",
    "    # Ball collision with right wall (rebound)\n",
    "    if ball_x + BALL_RADIUS >= WIDTH:\n",
    "        ball_speed_x *= -1\n",
    "\n",
    "    # Drawing the paddle in red\n",
    "    cv2.rectangle(frame, (0, player_y), (PADDLE_WIDTH, player_y + PADDLE_HEIGHT), (0, 0, 255), -1)\n",
    "    \n",
    "    # Drawing the ball in red\n",
    "    cv2.circle(frame, (ball_x, ball_y), BALL_RADIUS, (0, 0, 255), -1)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Pong Game', frame)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
