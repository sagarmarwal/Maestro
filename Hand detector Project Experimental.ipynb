{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a86b45f1-b2f1-4f11-9e97-69dac63b3dd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Example Counting Fingers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf092a77",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Step 1: Install dependencies (Run this cell first in Google Colab)\n",
    "!pip install mediapipe opencv-python\n",
    "\n",
    "# Step 2: Import necessary libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Step 3: Initialize MediaPipe Hands and Drawing Utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Step 4: Function to count raised fingers based on landmarks\n",
    "def count_fingers(hand_landmarks):\n",
    "    fingers = []\n",
    "\n",
    "    # Thumb\n",
    "    if hand_landmarks[4].x < hand_landmarks[3].x:\n",
    "        fingers.append(1)\n",
    "    else:\n",
    "        fingers.append(0)\n",
    "    \n",
    "    # Other four fingers\n",
    "    for tip in [8, 12, 16, 20]:\n",
    "        if hand_landmarks[tip].y < hand_landmarks[tip - 2].y:\n",
    "            fingers.append(1)\n",
    "        else:\n",
    "            fingers.append(0)\n",
    "\n",
    "    return fingers.count(1)\n",
    "\n",
    "# Step 5: Webcam setup and hand sign detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a later selfie-view display\n",
    "        # Convert the BGR image to RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Process the image and detect hand landmarks\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Convert the image back to BGR for OpenCV processing\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the image\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Count fingers\n",
    "                num_fingers = count_fingers(hand_landmarks.landmark)\n",
    "\n",
    "                # Display the number of fingers\n",
    "                cv2.putText(image, f'Fingers: {num_fingers}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display the final output\n",
    "        cv2.imshow('Hand Sign Detection', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ac2c1-152e-443c-9779-69c5ba97c930",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Example doing action when finger nb is detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789d9df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Install dependencies\n",
    "!pip install mediapipe opencv-python\n",
    "\n",
    "# Step 2: Import necessary libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Step 3: Initialize MediaPipe Hands and Drawing Utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Step 4: Define actions based on the number of fingers\n",
    "def perform_action(num_fingers):\n",
    "    if num_fingers == 1:\n",
    "        print(\"Action 1: Opening browser\")\n",
    "    elif num_fingers == 2:\n",
    "        print(\"Action 2: Playing a sound\")\n",
    "    elif num_fingers == 3:\n",
    "        print(\"Action 3: Opening a file\")\n",
    "    elif num_fingers == 4:\n",
    "        print(\"Action 4: Sending a message\")\n",
    "    elif num_fingers == 5:\n",
    "        print(\"Action 5: Showing a message\")\n",
    "\n",
    "# Step 5: Draw buttons on the screen\n",
    "def draw_buttons(image):\n",
    "    # Create buttons\n",
    "    cv2.rectangle(image, (50, 100), (250, 200), (0, 255, 0), -1)\n",
    "    cv2.putText(image, '1 Finger Action', (70, 150), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.rectangle(image, (50, 220), (250, 320), (0, 255, 0), -1)\n",
    "    cv2.putText(image, '2 Finger Action', (70, 270), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.rectangle(image, (50, 340), (250, 440), (0, 255, 0), -1)\n",
    "    cv2.putText(image, '3 Finger Action', (70, 390), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.rectangle(image, (50, 460), (250, 560), (0, 255, 0), -1)\n",
    "    cv2.putText(image, '4 Finger Action', (70, 510), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.rectangle(image, (50, 580), (250, 680), (0, 255, 0), -1)\n",
    "    cv2.putText(image, '5 Finger Action', (70, 630), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.8, (255, 255, 255), 2)\n",
    "\n",
    "# Step 6: Count raised fingers based on landmarks\n",
    "def count_fingers(hand_landmarks):\n",
    "    fingers = []\n",
    "\n",
    "    # Thumb\n",
    "    if hand_landmarks[4].x < hand_landmarks[3].x:\n",
    "        fingers.append(1)\n",
    "    else:\n",
    "        fingers.append(0)\n",
    "    \n",
    "    # Other four fingers\n",
    "    for tip in [8, 12, 16, 20]:\n",
    "        if hand_landmarks[tip].y < hand_landmarks[tip - 2].y:\n",
    "            fingers.append(1)\n",
    "        else:\n",
    "            fingers.append(0)\n",
    "\n",
    "    return fingers.count(1)\n",
    "\n",
    "# Step 7: Webcam setup and gesture detection with buttons\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display\n",
    "        # Convert the BGR image to RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Process the image and detect hand landmarks\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Convert the image back to BGR for OpenCV processing\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw buttons on the screen\n",
    "        draw_buttons(image)\n",
    "\n",
    "        # If hand landmarks are detected, count fingers and perform actions\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the image\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Count fingers\n",
    "                num_fingers = count_fingers(hand_landmarks.landmark)\n",
    "\n",
    "                # Perform action based on the number of fingers\n",
    "                perform_action(num_fingers)\n",
    "\n",
    "                # Display the number of fingers on the screen\n",
    "                cv2.putText(image, f'Fingers: {num_fingers}', (300, 100), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display the final output\n",
    "        cv2.imshow('Hand Gesture Detection with Buttons', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bfeddf-4932-49cd-b096-03de0882a521",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Example doing action when index on button with delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f581885c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Install dependencies\n",
    "!pip install mediapipe opencv-python\n",
    "\n",
    "# Step 2: Import necessary libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "# Step 3: Initialize MediaPipe Hands and Drawing Utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Step 4: Define actions when buttons are clicked\n",
    "def perform_action(button_name):\n",
    "    if button_name == '1 Finger Action':\n",
    "        print(\"Action 1: Opening browser\")\n",
    "    elif button_name == '2 Finger Action':\n",
    "        print(\"Action 2: Playing a sound\")\n",
    "    elif button_name == '3 Finger Action':\n",
    "        print(\"Action 3: Opening a file\")\n",
    "    elif button_name == '4 Finger Action':\n",
    "        print(\"Action 4: Sending a message\")\n",
    "    elif button_name == '5 Finger Action':\n",
    "        print(\"Action 5: Showing a message\")\n",
    "\n",
    "# Step 5: Draw buttons on the screen\n",
    "def draw_buttons(image):\n",
    "    # Create buttons\n",
    "    cv2.rectangle(image, (50, 100), (250, 200), (0, 255, 0), -1)  # Button 1\n",
    "    cv2.putText(image, '1 Finger Action', (70, 150), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.rectangle(image, (50, 220), (250, 320), (0, 255, 0), -1)  # Button 2\n",
    "    cv2.putText(image, '2 Finger Action', (70, 270), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.rectangle(image, (50, 340), (250, 440), (0, 255, 0), -1)  # Button 3\n",
    "    cv2.putText(image, '3 Finger Action', (70, 390), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.rectangle(image, (50, 460), (250, 560), (0, 255, 0), -1)  # Button 4\n",
    "    cv2.putText(image, '4 Finger Action', (70, 510), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.rectangle(image, (50, 580), (250, 680), (0, 255, 0), -1)  # Button 5\n",
    "    cv2.putText(image, '5 Finger Action', (70, 630), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.8, (255, 255, 255), 2)\n",
    "\n",
    "# Step 6: Check if the index finger is on a button\n",
    "def check_button_click(finger_x, finger_y):\n",
    "    # Button 1: 1 Finger Action\n",
    "    if 50 < finger_x < 250 and 100 < finger_y < 200:\n",
    "        return '1 Finger Action'\n",
    "    # Button 2: 2 Finger Action\n",
    "    elif 50 < finger_x < 250 and 220 < finger_y < 320:\n",
    "        return '2 Finger Action'\n",
    "    # Button 3: 3 Finger Action\n",
    "    elif 50 < finger_x < 250 and 340 < finger_y < 440:\n",
    "        return '3 Finger Action'\n",
    "    # Button 4: 4 Finger Action\n",
    "    elif 50 < finger_x < 250 and 460 < finger_y < 560:\n",
    "        return '4 Finger Action'\n",
    "    # Button 5: 5 Finger Action\n",
    "    elif 50 < finger_x < 250 and 580 < finger_y < 680:\n",
    "        return '5 Finger Action'\n",
    "    return None\n",
    "\n",
    "# Step 7: Draw a progress circle as visual feedback\n",
    "def draw_progress_circle(image, finger_x, finger_y, progress):\n",
    "    radius = 30\n",
    "    color = (0, 255, 0)\n",
    "    thickness = 2\n",
    "    cv2.circle(image, (finger_x, finger_y), radius, color, thickness)\n",
    "    cv2.ellipse(image, (finger_x, finger_y), (radius, radius), 0, 0, int(progress * 360), color, -1)\n",
    "\n",
    "# Step 8: Webcam setup and gesture detection with clickable buttons and timed clicks\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    button_hover_start_time = None  # Track when the finger starts hovering over a button\n",
    "    target_button = None  # The button currently being hovered over\n",
    "    hover_duration = 2  # Time in seconds to \"click\" the button\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display\n",
    "        # Convert the BGR image to RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Process the image and detect hand landmarks\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Convert the image back to BGR for OpenCV processing\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw buttons on the screen\n",
    "        draw_buttons(image)\n",
    "\n",
    "        # If hand landmarks are detected, check if the index finger is over a button\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the image\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Get the index finger tip coordinates (landmark 8)\n",
    "                index_finger_tip = hand_landmarks.landmark[8]\n",
    "                h, w, c = image.shape\n",
    "                finger_x = int(index_finger_tip.x * w)\n",
    "                finger_y = int(index_finger_tip.y * h)\n",
    "\n",
    "                # Check if the index finger is hovering over a button\n",
    "                current_button = check_button_click(finger_x, finger_y)\n",
    "\n",
    "                if current_button:\n",
    "                    # If hovering over the same button\n",
    "                    if current_button == target_button:\n",
    "                        # Check if enough time has passed to \"click\" the button\n",
    "                        elapsed_time = time.time() - button_hover_start_time\n",
    "                        progress = min(1.0, elapsed_time / hover_duration)  # Calculate progress (0 to 1)\n",
    "                        \n",
    "                        # Draw progress circle around the finger\n",
    "                        draw_progress_circle(image, finger_x, finger_y, progress)\n",
    "\n",
    "                        if elapsed_time >= hover_duration:\n",
    "                            perform_action(current_button)\n",
    "                            button_hover_start_time = None  # Reset hover time\n",
    "                            target_button = None  # Reset button target\n",
    "                    else:\n",
    "                        # Started hovering over a new button\n",
    "                        target_button = current_button\n",
    "                        button_hover_start_time = time.time()  # Start the timer\n",
    "                else:\n",
    "                    # Reset the timer if not hovering over any button\n",
    "                    button_hover_start_time = None\n",
    "                    target_button = None\n",
    "\n",
    "                # Draw a circle on the index finger tip\n",
    "                cv2.circle(image, (finger_x, finger_y), 10, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "        # Display the final output\n",
    "        cv2.imshow('Hand Gesture Detection with Clickable Buttons and Timed Click', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc8232-e56a-4dda-81a0-f0a0288b5e2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Example control mouse with finger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6abd56-9ac7-4f6b-b4f1-e27b2dcaf579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Install necessary libraries\n",
    "!pip install mediapipe opencv-python pyautogui\n",
    "# Step 2: Import required libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "\n",
    "# Step 3: Initialize MediaPipe Hands and Drawing Utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Step 4: Webcam setup and finger-based mouse control\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display\n",
    "        # Convert the BGR image to RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Process the image and detect hand landmarks\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Convert the image back to BGR for OpenCV processing\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # If hand landmarks are detected, move the mouse based on index finger position\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the image\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Get the index finger tip coordinates (landmark 8)\n",
    "                index_finger_tip = hand_landmarks.landmark[8]\n",
    "                h, w, c = image.shape\n",
    "                finger_x = int(index_finger_tip.x * w)\n",
    "                finger_y = int(index_finger_tip.y * h)\n",
    "\n",
    "                # Convert the finger coordinates from webcam resolution to screen resolution\n",
    "                screen_x = np.interp(finger_x, [0, w], [0, screen_width])\n",
    "                screen_y = np.interp(finger_y, [0, h], [0, screen_height])\n",
    "\n",
    "                # Move the mouse cursor to the finger position\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "                # Draw a circle on the index finger tip\n",
    "                cv2.circle(image, (finger_x, finger_y), 10, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "        # Display the webcam feed with hand tracking\n",
    "        cv2.imshow('Finger Mouse Control', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ad669-0098-408d-b95b-60cdab0a96f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Main menu with button to control mouse and button to show virtual keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916d2ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Install necessary libraries\n",
    "!pip install mediapipe opencv-python pyautogui\n",
    "\n",
    "# Step 2: Import required libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess  # To run processes in the background\n",
    "\n",
    "# Step 3: Initialize MediaPipe Hands and Drawing Utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Get screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Step 4: Button configuration\n",
    "# Variables to control toggling states\n",
    "mouse_control_enabled = False\n",
    "virtual_keyboard_enabled = False\n",
    "virtual_keyboard_process = None  # To store the virtual keyboard process\n",
    "\n",
    "# Step 5: Define actions for buttons\n",
    "def toggle_mouse_control():\n",
    "    global mouse_control_enabled\n",
    "    mouse_control_enabled = not mouse_control_enabled\n",
    "    print(f\"Mouse control enabled: {mouse_control_enabled}\")\n",
    "\n",
    "def toggle_virtual_keyboard():\n",
    "    global virtual_keyboard_enabled, virtual_keyboard_process\n",
    "    virtual_keyboard_enabled = not virtual_keyboard_enabled\n",
    "\n",
    "    if virtual_keyboard_enabled:\n",
    "        # Open virtual keyboard in the background\n",
    "        virtual_keyboard_process = subprocess.Popen('osk')  # Windows specific command\n",
    "    else:\n",
    "        # Close virtual keyboard (Windows specific)\n",
    "        if virtual_keyboard_process:\n",
    "            virtual_keyboard_process.terminate()  # Terminate the process\n",
    "        os.system('taskkill /IM osk.exe /F')  # Force close in case terminate doesn't work\n",
    "\n",
    "    print(f\"Virtual keyboard enabled: {virtual_keyboard_enabled}\")\n",
    "\n",
    "# Step 6: Draw the main menu buttons on the screen\n",
    "def draw_buttons(image, img_w, img_h):\n",
    "    # Button 1 (Top-left): Toggle Mouse Control\n",
    "    cv2.rectangle(image, (10, 10), (160, 80), (0, 255, 0), -1)  # Green button\n",
    "    cv2.putText(image, 'Mouse', (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Button 2 (Top-right): Toggle Virtual Keyboard\n",
    "    cv2.rectangle(image, (img_w - 160, 10), (img_w - 10, 80), (0, 0, 255), -1)  # Red button\n",
    "    cv2.putText(image, 'Keyboard', (img_w - 150, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "# Step 7: Check if the index finger is on a button\n",
    "def check_button_click(finger_x, finger_y, img_w, img_h):\n",
    "    if 10 < finger_x < 160 and 10 < finger_y < 80:  # Mouse control button\n",
    "        return 'mouse_control'\n",
    "    elif img_w - 160 < finger_x < img_w - 10 and 10 < finger_y < 80:  # Virtual keyboard button\n",
    "        return 'virtual_keyboard'\n",
    "    return None\n",
    "\n",
    "# Step 8: Draw a progress circle as visual feedback\n",
    "def draw_progress_circle(image, finger_x, finger_y, progress):\n",
    "    radius = 20\n",
    "    color = (0, 255, 255)  # Yellow for progress\n",
    "    thickness = 2\n",
    "    cv2.circle(image, (finger_x, finger_y), radius, color, thickness)\n",
    "    cv2.ellipse(image, (finger_x, finger_y), (radius, radius), 0, 0, int(progress * 360), color, -1)\n",
    "\n",
    "# Step 9: Webcam setup and main menu interaction\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    button_hover_start_time = None  # Track when the finger starts hovering over a button\n",
    "    target_button = None  # The button currently being hovered over\n",
    "    hover_duration = 2  # Time in seconds to \"click\" the button\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display\n",
    "        # Convert the BGR image to RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Process the image and detect hand landmarks\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Convert the image back to BGR for OpenCV processing\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        h, w, c = image.shape\n",
    "        # Draw the buttons on the screen\n",
    "        draw_buttons(image, w, h)\n",
    "\n",
    "        # If hand landmarks are detected, handle interactions with the buttons\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the image\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Get the index finger tip coordinates (landmark 8)\n",
    "                index_finger_tip = hand_landmarks.landmark[8]\n",
    "                finger_x = int(index_finger_tip.x * w)\n",
    "                finger_y = int(index_finger_tip.y * h)\n",
    "\n",
    "                # Check if the index finger is hovering over a button\n",
    "                current_button = check_button_click(finger_x, finger_y, w, h)\n",
    "\n",
    "                if current_button:\n",
    "                    # If hovering over the same button\n",
    "                    if current_button == target_button:\n",
    "                        # Check if enough time has passed to \"click\" the button\n",
    "                        elapsed_time = time.time() - button_hover_start_time\n",
    "                        progress = min(1.0, elapsed_time / hover_duration)  # Calculate progress (0 to 1)\n",
    "\n",
    "                        # Draw progress circle around the finger\n",
    "                        draw_progress_circle(image, finger_x, finger_y, progress)\n",
    "\n",
    "                        if elapsed_time >= hover_duration:\n",
    "                            if target_button == 'mouse_control':\n",
    "                                toggle_mouse_control()\n",
    "                            elif target_button == 'virtual_keyboard':\n",
    "                                toggle_virtual_keyboard()\n",
    "                            button_hover_start_time = None  # Reset hover time\n",
    "                            target_button = None  # Reset button target\n",
    "                    else:\n",
    "                        # Started hovering over a new button\n",
    "                        target_button = current_button\n",
    "                        button_hover_start_time = time.time()  # Start the timer\n",
    "                else:\n",
    "                    # Reset the timer if not hovering over any button\n",
    "                    button_hover_start_time = None\n",
    "                    target_button = None\n",
    "\n",
    "                # Draw a circle on the index finger tip\n",
    "                cv2.circle(image, (finger_x, finger_y), 10, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "                # If mouse control is enabled, move the mouse based on index finger position\n",
    "                if mouse_control_enabled:\n",
    "                    screen_x = np.interp(finger_x, [0, w], [0, screen_width])\n",
    "                    screen_y = np.interp(finger_y, [0, h], [0, screen_height])\n",
    "                    pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "        # Display the webcam feed with the main menu\n",
    "        cv2.imshow('Main Menu with Clickable Buttons', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda2dcf-34bc-4a25-b793-b3019d2aac4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Example volume control with fingers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6968944-7bb5-48a0-a5bf-0fbcc61251ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Install necessary libraries\n",
    "!pip install mediapipe opencv-python pycaw\n",
    "\n",
    "# Step 2: Import required libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "import numpy as np\n",
    "\n",
    "# Step 3: Initialize MediaPipe Hands and pycaw for volume control\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize pycaw for volume control\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "# Get volume range (min, max)\n",
    "vol_range = volume.GetVolumeRange()\n",
    "min_vol = vol_range[0]\n",
    "max_vol = vol_range[1]\n",
    "\n",
    "# Step 4: Function to calculate the distance between two points\n",
    "def calculate_distance(x1, y1, x2, y2):\n",
    "    return math.hypot(x2 - x1, y2 - y1)\n",
    "\n",
    "# Step 5: Webcam setup for hand detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Deadzone for setting volume to 0\n",
    "deadzone = 20  # Distance in pixels considered \"touching\" (you can adjust this)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display\n",
    "        # Convert the BGR image to RGB\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Process the image and detect hand landmarks\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Convert the image back to BGR for OpenCV processing\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # If hand landmarks are detected, process them\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the image\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Get thumb tip and index finger tip coordinates\n",
    "                thumb_tip = hand_landmarks.landmark[4]   # Thumb tip\n",
    "                index_finger_tip = hand_landmarks.landmark[8]  # Index finger tip\n",
    "\n",
    "                # Get height and width of the image\n",
    "                h, w, c = image.shape\n",
    "\n",
    "                # Convert normalized landmarks to pixel values\n",
    "                thumb_x, thumb_y = int(thumb_tip.x * w), int(thumb_tip.y * h)\n",
    "                index_x, index_y = int(index_finger_tip.x * w), int(index_finger_tip.y * h)\n",
    "\n",
    "                # Calculate the distance between thumb and index finger\n",
    "                distance = calculate_distance(thumb_x, thumb_y, index_x, index_y)\n",
    "\n",
    "                # Set volume to 0 if the distance is within the deadzone\n",
    "                if distance < deadzone:\n",
    "                    vol = min_vol  # Set to minimum volume\n",
    "                else:\n",
    "                    # Map the distance to volume (0 when touching, max when far apart)\n",
    "                    max_distance = 200  # Adjust this based on your hand size and webcam resolution\n",
    "                    vol = np.interp(distance, [deadzone, max_distance], [min_vol, max_vol])\n",
    "\n",
    "                # Set the system volume based on the calculated volume\n",
    "                volume.SetMasterVolumeLevel(vol, None)\n",
    "\n",
    "                # Draw circles on thumb and index finger tips\n",
    "                cv2.circle(image, (thumb_x, thumb_y), 10, (255, 0, 0), cv2.FILLED)\n",
    "                cv2.circle(image, (index_x, index_y), 10, (255, 0, 0), cv2.FILLED)\n",
    "\n",
    "                # Draw a line between thumb and index finger\n",
    "                cv2.line(image, (thumb_x, thumb_y), (index_x, index_y), (0, 255, 0), 3)\n",
    "\n",
    "                # Display the distance (for debugging purposes)\n",
    "                cv2.putText(image, f'Distance: {int(distance)}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "                # Display the volume level (for debugging purposes)\n",
    "                cv2.putText(image, f'Volume: {int(np.interp(vol, [min_vol, max_vol], [0, 100]))}%', \n",
    "                            (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        # Display the webcam feed\n",
    "        cv2.imshow('Volume Control with Hand Gestures', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5bb761-568c-48ae-b67e-67fb83ebf993",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Swipe up for open apps, down for desktop, left right for switching apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562dc1d4-8fab-4fe8-bd2b-78e869c5f0d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Install necessary libraries\n",
    "!pip install mediapipe opencv-python pyautogui\n",
    "\n",
    "# Step 2: Import required libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "\n",
    "# Step 3: Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Step 4: Webcam setup for hand detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Gesture recognition variables\n",
    "swipe_threshold = 100  # Distance in pixels to recognize a swipe\n",
    "last_swipe_time = 0  # Time of last swipe\n",
    "swipe_timeout = 1  # Time in seconds to limit swipes\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    last_position = None  # To track the last hand position\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Process the image and detect hand landmarks\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Convert the image back to BGR for OpenCV processing\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # If hand landmarks are detected, process them\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the image\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Get index finger tip coordinates\n",
    "                index_finger_tip = hand_landmarks.landmark[8]  # Index finger tip\n",
    "\n",
    "                # Get height and width of the image\n",
    "                h, w, c = image.shape\n",
    "\n",
    "                # Convert normalized landmark to pixel values\n",
    "                index_x, index_y = int(index_finger_tip.x * w), int(index_finger_tip.y * h)\n",
    "\n",
    "                # Calculate the swipe direction based on previous position\n",
    "                if last_position is not None:\n",
    "                    distance_y = last_position[1] - index_y  # Difference in Y position\n",
    "                    distance_x = last_position[0] - index_x  # Difference in X position\n",
    "\n",
    "                    # Check if the swipe distance is significant\n",
    "                    if abs(distance_y) > swipe_threshold:\n",
    "                        current_time = time.time()\n",
    "                        if current_time - last_swipe_time > swipe_timeout:\n",
    "                            last_swipe_time = current_time\n",
    "                            if distance_y > swipe_threshold:  # Swipe Up\n",
    "                                pyautogui.hotkey('win', 'tab')  # Show all apps\n",
    "                            elif distance_y < -swipe_threshold:  # Swipe Down\n",
    "                                pyautogui.hotkey('win', 'd')  # Show desktop\n",
    "\n",
    "                    # Check left/right swipe\n",
    "                    if abs(distance_x) > swipe_threshold:\n",
    "                        current_time = time.time()\n",
    "                        if current_time - last_swipe_time > swipe_timeout:\n",
    "                            last_swipe_time = current_time\n",
    "                            if distance_x > swipe_threshold:  # Swipe Right\n",
    "                                pyautogui.hotkey('alt', 'tab')  # Switch to the next app\n",
    "                            elif distance_x < -swipe_threshold:  # Swipe Left\n",
    "                                pyautogui.hotkey('shift', 'alt', 'tab')  # Switch to the previous app\n",
    "\n",
    "                # Update the last position\n",
    "                last_position = (index_x, index_y)\n",
    "\n",
    "        # Display the webcam feed\n",
    "        cv2.imshow('Gesture Control for Apps', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:  # Press 'Esc' to exit\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279fc7dd-4476-41a9-b496-7bf7e6ff3c9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pong game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b33f5-7563-4a53-a971-dfb1ca6ac286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialize Mediapipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Pong game parameters\n",
    "WIDTH, HEIGHT = 640, 480\n",
    "PADDLE_WIDTH, PADDLE_HEIGHT = 20, 100\n",
    "BALL_RADIUS = 10\n",
    "PADDLE_SPEED = 20\n",
    "ball_speed_x, ball_speed_y = 7, 7\n",
    "\n",
    "# Initialize paddle and ball positions\n",
    "player_y = HEIGHT // 2 - PADDLE_HEIGHT // 2\n",
    "ball_x, ball_y = WIDTH // 2, HEIGHT // 2\n",
    "\n",
    "# Create a function to detect finger position\n",
    "def detect_finger_position(frame):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(frame_rgb)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Get index finger tip position (landmark 8)\n",
    "            x = int(hand_landmarks.landmark[8].x * WIDTH)\n",
    "            y = int(hand_landmarks.landmark[8].y * HEIGHT)\n",
    "            return x, y\n",
    "    return None, None\n",
    "\n",
    "# Game loop\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip frame for natural movement\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Detect finger position\n",
    "    finger_x, finger_y = detect_finger_position(frame)\n",
    "\n",
    "    # Move the paddle\n",
    "    if finger_y is not None:\n",
    "        player_y = finger_y - PADDLE_HEIGHT // 2\n",
    "\n",
    "    # Bound the paddle within the screen\n",
    "    if player_y < 0:\n",
    "        player_y = 0\n",
    "    if player_y + PADDLE_HEIGHT > HEIGHT:\n",
    "        player_y = HEIGHT - PADDLE_HEIGHT\n",
    "\n",
    "    # Ball movement\n",
    "    ball_x += ball_speed_x\n",
    "    ball_y += ball_speed_y\n",
    "\n",
    "    # Ball collision with top and bottom\n",
    "    if ball_y - BALL_RADIUS <= 0 or ball_y + BALL_RADIUS >= HEIGHT:\n",
    "        ball_speed_y *= -1\n",
    "\n",
    "    # Ball collision with player paddle\n",
    "    if (ball_x - BALL_RADIUS <= PADDLE_WIDTH and \n",
    "        player_y <= ball_y <= player_y + PADDLE_HEIGHT):\n",
    "        ball_speed_x *= -1\n",
    "\n",
    "    # Ball collision with right wall (rebound)\n",
    "    if ball_x + BALL_RADIUS >= WIDTH:\n",
    "        ball_speed_x *= -1\n",
    "\n",
    "    # Drawing the paddle in red\n",
    "    cv2.rectangle(frame, (0, player_y), (PADDLE_WIDTH, player_y + PADDLE_HEIGHT), (0, 0, 255), -1)\n",
    "    \n",
    "    # Drawing the ball in red\n",
    "    cv2.circle(frame, (ball_x, ball_y), BALL_RADIUS, (0, 0, 255), -1)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Pong Game', frame)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
